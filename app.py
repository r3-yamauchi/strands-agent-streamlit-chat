import asyncio
import copy
import glob
import json
import os
import time
from contextlib import ExitStack
from pathlib import Path
from typing import cast, get_args

import boto3
import nest_asyncio
import streamlit as st
import yaml
from mcp import StdioServerParameters, stdio_client
from strands import Agent
from strands.models import BedrockModel
from strands.models.openai import OpenAIModel
from strands.tools.mcp import MCPClient
from strands.tools.mcp.mcp_agent_tool import MCPAgentTool
from strands.types.content import ContentBlock, Message, Messages
from strands.types.media import ImageFormat
from strands_tools import current_time, http_request
from dotenv import load_dotenv
load_dotenv()

nest_asyncio.apply()

os.environ["DEV"] = "true"

format = {"image": list(get_args(ImageFormat))}

builtin_tools = [current_time, http_request]


async def streaming(stream):
    """
    ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°ãƒ‡ãƒ¼ã‚¿ã‚’å‡¦ç†ã—ã€ãƒ†ã‚­ã‚¹ãƒˆã¾ãŸã¯ãƒ„ãƒ¼ãƒ«ä½¿ç”¨æƒ…å ±ã‚’ç”Ÿæˆã™ã‚‹éåŒæœŸã‚¸ã‚§ãƒãƒ¬ãƒ¼ã‚¿é–¢æ•°ã€‚

    Args:
        stream: ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã‹ã‚‰ã®ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°ãƒ¬ã‚¹ãƒãƒ³ã‚¹

    Yields:
        str: ãƒ†ã‚­ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã¾ãŸã¯ãƒ„ãƒ¼ãƒ«ä½¿ç”¨æƒ…å ±ã®ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆæ¸ˆã¿æ–‡å­—åˆ—
    """
    async for event in stream:
        # ã‚¤ãƒ™ãƒ³ãƒˆã«ãƒ‡ãƒ¼ã‚¿ãŒå«ã¾ã‚Œã¦ã„ã‚‹å ´åˆã€ãƒ†ã‚­ã‚¹ãƒˆã¨ã—ã¦å‡ºåŠ›
        if "data" in event:
            # ãƒ†ã‚­ã‚¹ãƒˆã‚’å‡ºåŠ›
            data = event["data"]
            yield data
        # ã‚¤ãƒ™ãƒ³ãƒˆã«ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ãŒå«ã¾ã‚Œã¦ã„ã‚‹å ´åˆã€ãƒ„ãƒ¼ãƒ«ä½¿ç”¨æƒ…å ±ã‚’æŠ½å‡ºã—ã¦å‡ºåŠ›
        elif "message" in event:
            # ToolUseã‚’å‡ºåŠ›
            message: Message = event["message"]
            # ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã®å†…å®¹ã‹ã‚‰ãƒ„ãƒ¼ãƒ«ä½¿ç”¨æƒ…å ±ã‚’æŠ½å‡º
            for content in [
                content for content in message["content"] if "toolUse" in content.keys()
            ]:
                yield f"\n\nğŸ”§ Using tool:\n```json\n{json.dumps(content, indent=2, ensure_ascii=False)}\n```\n\n"


def convert_messages(messages: Messages, enable_cache: bool):
    """
    ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸å±¥æ­´ã«ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒã‚¤ãƒ³ãƒˆã‚’è¿½åŠ ã™ã‚‹é–¢æ•°ã€‚

    Args:
        messages (Messages): å¤‰æ›ã™ã‚‹ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸å±¥æ­´
        enable_cache (bool): ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚’æœ‰åŠ¹ã«ã™ã‚‹ã‹ã©ã†ã‹ã®ãƒ•ãƒ©ã‚°

    Returns:
        Messages: ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒã‚¤ãƒ³ãƒˆãŒè¿½åŠ ã•ã‚ŒãŸãƒ¡ãƒƒã‚»ãƒ¼ã‚¸å±¥æ­´
    """
    messages_with_cache_point: Messages = []
    user_turns_processed = 0

    for message in reversed(messages):
        m = copy.deepcopy(message)

        if enable_cache:
            if message["role"] == "user" and user_turns_processed < 2:
                if len([c for c in m["content"] if "text" in c]) > 0:
                    m["content"].append({"cachePoint": {"type": "default"}})  # type: ignore
                    user_turns_processed += 1
                else:
                    pass

        messages_with_cache_point.append(m)

    messages_with_cache_point.reverse()

    return messages_with_cache_point


def initialize_model(provider: str, model_id: str, config: dict, enable_cache: dict):
    """
    ãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼ã«å¿œã˜ãŸãƒ¢ãƒ‡ãƒ«ã‚’åˆæœŸåŒ–
    
    Args:
        provider: ãƒ¢ãƒ‡ãƒ«ãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼ ("openai" or "bedrock")
        model_id: ãƒ¢ãƒ‡ãƒ«ID
        config: è¨­å®šè¾æ›¸
        enable_cache: ã‚­ãƒ£ãƒƒã‚·ãƒ¥è¨­å®š
    
    Returns:
        åˆæœŸåŒ–ã•ã‚ŒãŸãƒ¢ãƒ‡ãƒ«ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹
    """
    if provider == "openai":
        api_key = os.getenv("OPENAI_API_KEY")
        if not api_key:
            st.error("ğŸ”‘ OPENAI_API_KEY ãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚.env ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚")
            st.stop()
            
        model_config = config["providers"]["openai"]["models"].get(model_id, {})
        return OpenAIModel(
            client_args={"api_key": api_key},
            model_id=model_id,
            params={
                "max_tokens": model_config.get("max_tokens", 4000),
                "temperature": st.session_state.get("temperature", 0.7)
            }
        )
    
    elif provider == "bedrock":
        bedrock_region = config["providers"]["bedrock"]["region"]
        return BedrockModel(
            model_id=model_id,
            boto_session=boto3.Session(region_name=bedrock_region),
            cache_prompt="default" if enable_cache.get("system") else None,
            cache_tools="default" if enable_cache.get("tools") else None,
        )
    
    else:
        raise ValueError(f"Unknown provider: {provider}")


def get_model_capabilities(provider: str, model_id: str, config: dict) -> dict:
    """
    ãƒ¢ãƒ‡ãƒ«ã®æ©Ÿèƒ½ã‚’å–å¾—
    
    Args:
        provider: ãƒ¢ãƒ‡ãƒ«ãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼
        model_id: ãƒ¢ãƒ‡ãƒ«ID
        config: è¨­å®šè¾æ›¸
    
    Returns:
        ãƒ¢ãƒ‡ãƒ«ã®æ©Ÿèƒ½è¾æ›¸
    """
    if provider == "openai":
        model_config = config["providers"]["openai"]["models"].get(model_id, {})
        return {
            "image_support": model_config.get("image_support", False),
            "cache_support": [],  # OpenAIã¯ã‚­ãƒ£ãƒƒã‚·ãƒ¥éå¯¾å¿œ
            "streaming": True,
            "tools": True
        }
    elif provider == "bedrock":
        model_config = config["providers"]["bedrock"]["models"].get(model_id, {})
        return {
            "image_support": model_config.get("image_support", False),
            "cache_support": model_config.get("cache_support", []),
            "streaming": True,
            "tools": True
        }
    else:
        return {}


async def main():
    """
    Streamlitã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³ã®ãƒ¡ã‚¤ãƒ³é–¢æ•°ã€‚ãƒãƒ£ãƒƒãƒˆã‚¤ãƒ³ã‚¿ãƒ¼ãƒ•ã‚§ãƒ¼ã‚¹ã®è¨­å®šã€
    ãƒ¢ãƒ‡ãƒ«ã®åˆæœŸåŒ–ã€ãƒ¦ãƒ¼ã‚¶ãƒ¼å…¥åŠ›ã®å‡¦ç†ã€ãŠã‚ˆã³ãƒãƒ£ãƒƒãƒˆå±¥æ­´ã®ç®¡ç†ã‚’è¡Œã†ã€‚

    Returns:
        None
    """
    st.title("Strands agent")

    with open("config/config.json", "r") as f:
        config = json.load(f)

    # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼ã®è¨­å®š
    default_provider = os.getenv("DEFAULT_PROVIDER", "openai").lower()

    def select_chat(chat_history_file):
        st.session_state.chat_history_file = chat_history_file

    with st.sidebar:
        with st.expander(":gear: config", expanded=True):
            # ãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼é¸æŠ
            provider = st.selectbox(
                "ğŸ¢ Provider",
                ["OpenAI", "Amazon Bedrock"],
                index=0 if default_provider == "openai" else 1,
                key="provider"
            )
            
            # ãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼åˆ¥ãƒ¢ãƒ‡ãƒ«è¡¨ç¤º
            provider_key = provider.lower().replace(" ", "")
            if provider_key == "amazonbedrock":
                provider_key = "bedrock"
            
            if provider_key in config["providers"]:
                models = config["providers"][provider_key]["models"]
                model_id = st.selectbox(
                    "ğŸ¤– Model",
                    list(models.keys()),
                    format_func=lambda x: models[x].get("display_name", x),
                    key="model_id"
                )
                
                # ãƒ¢ãƒ‡ãƒ«ã®æ©Ÿèƒ½ã‚’å–å¾—
                capabilities = get_model_capabilities(provider_key, model_id, config)
                
                # OpenAIç‰¹æœ‰ã®è¨­å®š
                if provider == "OpenAI":
                    st.slider("Temperature", 0.0, 2.0, 0.7, 0.1, key="temperature")
                    if not capabilities["cache_support"]:
                        st.info("â„¹ï¸ OpenAIãƒ¢ãƒ‡ãƒ«ã¯ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚’ã‚µãƒãƒ¼ãƒˆã—ã¦ã„ã¾ã›ã‚“")
                
                # Bedrockç‰¹æœ‰ã®è¨­å®š
                if provider == "Amazon Bedrock" and capabilities["cache_support"]:
                    st.checkbox("Enable prompt cache", value=True, key="enable_prompt_cache")
            else:
                st.error(f"Provider {provider} not configured")

            chat_history_dir = st.text_input(
                "chat_history_dir", value=config["chat_history_dir"]
            )

            st.text_input(
                "mcp_config_file",
                value=config["mcp_config_file"],
                key="mcp_config_file",
            )

            with open(st.session_state.mcp_config_file, "r") as f:
                mcp_config = json.load(f)["mcpServers"]

            if "multiselect_mcp_tools" not in st.session_state:
                with st.spinner("Tool loading..."):
                    mcp_tools: list[MCPAgentTool] = []
                    with ExitStack() as stack:
                        for _, server_config in mcp_config.items():
                            server_parameter = StdioServerParameters(
                                command=server_config["command"],
                                args=server_config["args"],
                                env=server_config["env"]
                                if "env" in server_config
                                else None,
                            )
                            mcp_client = MCPClient(
                                lambda param=server_parameter: stdio_client(
                                    server=param
                                )
                            )
                            stack.enter_context(mcp_client)  # type: ignore
                            mcp_tools.extend(mcp_client.list_tools_sync())

                    st.session_state.multiselect_mcp_tools = [
                        tool.tool_name for tool in mcp_tools
                    ]

            st.pills(
                "MCP Tool",
                st.session_state.multiselect_mcp_tools,
                selection_mode="multi",
                default=st.session_state.multiselect_mcp_tools,
                key="selected_mcp_tools",
            )

        st.button(
            "New Chat",
            on_click=select_chat,
            args=(f"{chat_history_dir}/{int(time.time())}.yaml",),
            use_container_width=True,
            type="primary",
        )

    if "chat_history_file" not in st.session_state:
        st.session_state["chat_history_file"] = (
            f"{chat_history_dir}/{int(time.time())}.yaml"
        )
    chat_history_file = st.session_state.chat_history_file

    if Path(chat_history_file).exists():
        with open(chat_history_file, mode="rt") as f:
            yaml_msg = yaml.safe_load(f)
            messages: Messages = yaml_msg
    else:
        messages: Messages = []

    for message in messages:
        for content in message["content"]:
            with st.chat_message(message["role"]):
                if "text" in content:
                    st.write(content["text"])
                elif "image" in content:
                    st.image(content["image"]["source"]["bytes"])

    # ãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼ã®å–å¾—
    provider_name = st.session_state.get("provider", "OpenAI")
    provider_key = provider_name.lower().replace(" ", "")
    if provider_key == "amazonbedrock":
        provider_key = "bedrock"
    
    # ãƒ¢ãƒ‡ãƒ«æ©Ÿèƒ½ã®å–å¾—
    capabilities = get_model_capabilities(provider_key, st.session_state.model_id, config)
    
    enable_prompt_cache_system = False
    enable_prompt_cache_tools = False
    enable_prompt_cache_messages = False

    # Bedrockã®å ´åˆã®ã¿ã‚­ãƒ£ãƒƒã‚·ãƒ¥è¨­å®šã‚’ç¢ºèª
    if provider_key == "bedrock" and st.session_state.get("enable_prompt_cache", False):
        cache_support = capabilities.get("cache_support", [])
        enable_prompt_cache_system = True if "system" in cache_support else False
        enable_prompt_cache_tools = True if "tools" in cache_support else False
        enable_prompt_cache_messages = True if "messages" in cache_support else False

    image_support: bool = capabilities.get("image_support", False)

    if prompt := st.chat_input(accept_file="multiple", file_type=format["image"]):
        with st.chat_message("user"):
            st.write(prompt.text)
            for file in prompt.files:
                if image_support:
                    st.image(file.getvalue())
                else:
                    st.warning(
                        "ã“ã®ãƒ¢ãƒ‡ãƒ«ã¯ç”»åƒã¯ã‚µãƒãƒ¼ãƒˆã—ã¦ã„ã¾ã›ã‚“ã€‚ç”»åƒã¯ä½¿ç”¨ã•ã‚Œã¾ã›ã‚“ã€‚"
                    )

        if prompt.files and image_support:
            image_content: list[ContentBlock] = []
            for file in prompt.files:
                if (file_format := file.type.split("/")[1]) in format["image"]:
                    image_content.append(
                        {
                            "image": {
                                "format": cast(ImageFormat, file_format),
                                "source": {"bytes": file.getvalue()},
                            }
                        }
                    )
            messages = messages + [
                {"role": "user", "content": image_content},
                {
                    "role": "assistant",
                    "content": [
                        {"text": "I will reference this media in my next response."}
                    ],
                },
            ]

        # ExitStackã‚’ä½¿ç”¨ã—ã¦å¯å¤‰æ•°ã®ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã‚’ç®¡ç†
        with ExitStack() as stack:
            mcp_tools: list[MCPAgentTool] = []

            for server_name, server_config in mcp_config.items():
                server_parameter = StdioServerParameters(
                    command=server_config["command"],
                    args=server_config["args"],
                    env=server_config["env"] if "env" in server_config else None,
                )
                mcp_client = MCPClient(
                    lambda param=server_parameter: stdio_client(server=param)
                )
                stack.enter_context(mcp_client)  # type: ignore
                mcp_tools.extend(mcp_client.list_tools_sync())

                mcp_tools = [
                    tool
                    for tool in mcp_tools
                    if tool.tool_name in st.session_state.selected_mcp_tools
                ]

            tools = mcp_tools + builtin_tools

            # ãƒ¢ãƒ‡ãƒ«ã®åˆæœŸåŒ–
            model = initialize_model(
                provider=provider_key,
                model_id=st.session_state.model_id,
                config=config,
                enable_cache={
                    "system": enable_prompt_cache_system,
                    "tools": enable_prompt_cache_tools
                }
            )
            
            agent = Agent(
                model=model,
                system_prompt="ã‚ãªãŸã¯å„ªç§€ãªAIã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã§ã™ï¼",
                messages=convert_messages(
                    messages, enable_cache=enable_prompt_cache_messages
                ),
                callback_handler=None,
                tools=tools,
            )

            agent_stream = agent.stream_async(prompt=prompt.text)

            with st.chat_message("assistant"):
                st.write_stream(streaming(agent_stream))

            with open(chat_history_file, mode="wt") as f:
                yaml.safe_dump(agent.messages, f, allow_unicode=True)

    with st.sidebar:
        history_files = glob.glob(os.path.join(chat_history_dir, "*.yaml"))  # type: ignore

        for h in sorted(history_files, reverse=True)[:20]:  # latest 20
            st.button(h, on_click=select_chat, args=(h,), use_container_width=True)


if __name__ == "__main__":
    loop = asyncio.get_event_loop()
    loop.run_until_complete(main())
